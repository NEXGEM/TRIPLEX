{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIPLEX Data Processing Tutorial\n",
    "\n",
    "This tutorial provides a comprehensive guide to using the TRIPLEX pipeline for processing spatial transcriptomics (ST) data integrated with whole slide images (WSI). The pipeline supports various tasks including data preprocessing, feature extraction, and data preparation for both training and inference.\n",
    "\n",
    "## Overview of the Pipeline\n",
    "\n",
    "The TRIPLEX pipeline integrates several key components:\n",
    "\n",
    "1. **Data Preprocessing**: Preparing spatial transcriptomics data and whole slide images\n",
    "2. **Feature Extraction**: Extracting features from WSI patches using deep learning models\n",
    "3. **Dataset Preparation**: Creating datasets suitable for training and inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'tutorials':\n",
    "    # Change to parent directory\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.pipeline import TriplexPipeline, get_config                        \n",
    "from src.preprocess.pipeline.utils import get_available_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Check GPU Availability\n",
    "\n",
    "The pipeline can leverage GPU acceleration for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "available_gpus = get_available_gpus()\n",
    "print(f\"Available GPUs: {len(available_gpus)}\")\n",
    "\n",
    "# If running on GPU, show CUDA information\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Detected GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath('./src/preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the default configuration\n",
    "try:\n",
    "    default_config = get_config(f\"{working_dir}\", 'default')\n",
    "    print(\"Default configuration parameters:\")\n",
    "    for key, value in sorted(default_config.items()):\n",
    "        print(f\"  {key}: {value}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Default configuration file not found. Make sure to create it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating a Custom Configuration\n",
    "\n",
    "You can create a custom configuration by merging dictionaries or loading from a YAML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom configuration by overriding default values\n",
    "custom_config = {\n",
    "    'mode': 'train',\n",
    "    'platform': 'visium',\n",
    "    'input_dir': '/path/to/input/data',\n",
    "    'output_dir': '/path/to/output/data',\n",
    "    'slide_ext': '.svs',\n",
    "    'patch_size': 224,\n",
    "    'slide_level': 0,\n",
    "    'save_neighbors': True,\n",
    "    'total_gpus': min(2, len(available_gpus))  # Use at most 2 GPUs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Complete Pipeline Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Processing HEST data (Andersson; BC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YOUR HUGGING FACE TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processing HEST data\n",
    "hest_config = {\n",
    "    'mode': 'hest',\n",
    "    'input_dir': './input/ST/andersson',  \n",
    "    'output_dir': './input/ST/andersson', \n",
    "    'slide_ext': '.tif',\n",
    "    'save_neighbors': True,\n",
    "    'model_name': 'cigar'\n",
    "}\n",
    "\n",
    "pipeline = TriplexPipeline(hest_config)\n",
    "pipeline.run_pipeline()  # This will run preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Processing Visium data (GSE240429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processing Visium data for training\n",
    "visium_config = {\n",
    "    'mode': 'train',\n",
    "    'platform': 'visium',\n",
    "    'input_dir': '/data-hdd/home/shared/spRNAseq/public/GSE240429/',  # Replace with actual path\n",
    "    'output_dir': 'input/GSE240429',\n",
    "    'slide_ext': '.tif',\n",
    "    'save_neighbors': True\n",
    "}\n",
    "\n",
    "pipeline = TriplexPipeline(visium_config)\n",
    "pipeline.run_pipeline() # This will run preprocessing and feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Inference on WSI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of inference on WSI data\n",
    "inference_config = {\n",
    "    'mode': 'inference',\n",
    "    'input_dir': '/path/to/wsi/data',  # Replace with actual path\n",
    "    'output_dir': '/input/data/path',  # Replace with actual path\n",
    "    'slide_ext': '.mrxs',  # Aperio format\n",
    "    'slide_level': 1,  # Use level 1 for faster processing\n",
    "    'total_gpus': min(4, len(available_gpus))  # Use up to 4 GPUs\n",
    "}\n",
    "\n",
    "pipeline = TriplexPipeline(inference_config)\n",
    "pipeline.run_pipeline() # This will run preprocessing and feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Step by step processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Processing HEST data (Andersson; BC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Data Preprocessing\n",
    "\n",
    "The first step is preprocessing the data. This includes:\n",
    "- Loading and processing spatial transcriptomics data\n",
    "- Extracting patches from whole slide images\n",
    "- Preparing gene sets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Understanding Preprocessing Modes***\n",
    "\n",
    "TRIPLEX supports three main preprocessing modes:\n",
    "\n",
    "1. **train mode**: Used to prepare data for model training\n",
    "   - Processes both spatial transcriptomics data and WSIs\n",
    "   - Creates patch datasets from WSIs\n",
    "   - Extracts gene sets (highly variable genes and highly expressed genes)\n",
    "   - Splits data for cross-validation\n",
    "\n",
    "2. **hest mode**: Used for HEST (Histology-Enhanced Spatial Transcriptomics) data\n",
    "   - Loads pre-processed HEST data\n",
    "   - Extracts patches and neighbor information\n",
    "   - Prepares gene sets for training\n",
    "\n",
    "3. **inference mode**: Used to prepare data for inference\n",
    "   - Processes WSIs only (no spatial transcriptomics data required)\n",
    "   - Extracts patches and coordinates for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processing HEST data\n",
    "hest_config = {\n",
    "    'mode': 'hest',\n",
    "    'input_dir': './input/ST/andersson',  \n",
    "    'output_dir': './input/ST/andersson', \n",
    "    'slide_ext': '.tif',\n",
    "    'save_neighbors': True,\n",
    "    'model_name': 'cigar'\n",
    "}\n",
    "\n",
    "pipeline = TriplexPipeline(hest_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Feature Extraction\n",
    "\n",
    "After preprocessing, the next step is to extract features from the WSI patches. TRIPLEX extracts two types of features:\n",
    "\n",
    "1. **Global features**: Features extracted from individual patches\n",
    "2. **Neighbor features**: Features that incorporate neighborhood context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feature Extraction Models***\n",
    "\n",
    "TRIPLEX supports several feature extraction models:\n",
    "\n",
    "- **cigar**: A self-supervised model trained on histopathology images\n",
    "- Other models from the HEST model zoo can also be used\n",
    "\n",
    "The default is `cigar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-1) Sequential Feature Extraction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.run_extraction('global')  # Extract only global features\n",
    "# pipeline.run_extraction('neighbor')  # Extract only neighbor features\n",
    "pipeline.run_extraction('both')  # Extract both global and neighbor features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-2) Parallel Feature Extraction**\n",
    "\n",
    "For large datasets, TRIPLEX can perform feature extraction in parallel across multiple GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for parallel feature extraction\n",
    "parallel_config = {\n",
    "    'mode': 'hest',\n",
    "    'input_dir': './input/ST/andersson',  \n",
    "    'output_dir': './input/ST/andersson', \n",
    "    'slide_ext': '.tif',\n",
    "    'save_neighbors': True,\n",
    "    'model_name': 'cigar',\n",
    "    'total_gpus': len(available_gpus),  # Use all available GPUs\n",
    "    'batch_size': 1024,\n",
    "    'num_workers': 4\n",
    "}\n",
    "\n",
    "print(f\"Parallel extraction would use {len(available_gpus)} GPUs\")\n",
    "pipeline = TriplexPipeline(parallel_config)\n",
    "pipeline.run_parallel_extraction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the Command Line Interface\n",
    "\n",
    "The TRIPLEX pipeline can also be run from the command line using predefined configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Run the pipeline using a predefined configuration\n",
    "python script/run_pipeline.py BC1\n",
    "\n",
    "# Override specific parameters\n",
    "python script/run_pipeline.py inference --input_dir /path/to/data --total_gpus 2\n",
    "\n",
    "# Extract features in parallel\n",
    "bash script/extract_features_parallel.sh /path/to/images /path/to/output .svs 4 both\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Output Structure\n",
    "\n",
    "The TRIPLEX pipeline generates several output directories and files. Here's a guide to the output structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "output_dir/\n",
    "├── patches/           # Extracted patches from WSIs\n",
    "│   └── neighbor/      # Neighbor patches (if save_neighbors is True)\n",
    "├── adata/             # Processed gene expression data\n",
    "│   └── *.h5ad         # AnnData files with gene expression\n",
    "├── emb/               # Extracted features\n",
    "│   ├── global/        # Global features\n",
    "│   │   └── cigar/     # Features from the cigar model\n",
    "│   └── neighbor/      # Neighbor features\n",
    "│       └── cigar/     # Features from the cigar model\n",
    "├── pos/               # Patch positions for inference\n",
    "├── var_50genes.json   # Highly variable genes\n",
    "└── mean_1000genes.json # Highly expressed genes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Tips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Memory Management\n",
    "\n",
    "- Adjust `batch_size` based on your GPU memory\n",
    "- Use `num_workers` based on your CPU cores (typically 4-8 is sufficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Performance Optimization\n",
    "\n",
    "- Use multiple GPUs for feature extraction on large datasets\n",
    "- For very large datasets, process files in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Quality Control\n",
    "\n",
    "- Check intermediate outputs (patches, gene sets) to ensure quality\n",
    "- If feature extraction fails, try reducing batch size\n",
    "- Verify that gene expression data properly aligns with WSI patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Memory Errors\n",
    "\n",
    "If you encounter CUDA out of memory errors:\n",
    "- Reduce batch size\n",
    "- Process fewer files simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 File Format Issues\n",
    "\n",
    "If you have issues with file formats:\n",
    "- Ensure your slide extension matches the actual files\n",
    "- For Aperio SVS files, use '.svs'\n",
    "- For MRXS files, use '.mrxs'\n",
    "- For TIFF files, use '.tif' or '.tiff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Platform-Specific Issues\n",
    "\n",
    "For platform-specific preprocessing:\n",
    "- Visium data should have the standard 10X Visium folder structure\n",
    "- For HEST data, ensure the data is formatted according to HEST specifications\n",
    "- For custom platforms, you may need to modify the data loading functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRIPLEX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
